---
title: "Week 5: Bayesian linear regression and introduction to Stan"
date: today
date-format: "DD/MM/YY"
format: pdf
name: Xuanze Li
execute: 
  warning: false
  message: false
---

# Introduction

Today we will be starting off using Stan, looking at the kid's test score data set (available in resources for the [Gelman Hill textbook](https://mc-stan.org/rstanarm/reference/rstanarm-datasets.html)). 

```{r}
library(tidyverse)
library(rstan)
library(tidybayes)
library(here)
```


The data look like this:

```{r}
kidiq <- read_rds(here("kidiq.RDS"))
kidiq
```
As well as the kid's test scores, we have a binary variable indicating whether or not the mother completed high school, the mother's IQ and age. 

# Descriptives

## Question 1

Use plots or tables to show three interesting observations about the data. Remember:

- Explain what your graph/ tables show
- Choose a graph type that's appropriate to the data type

I first find it would be interesting to see how does the distribution of kid's test score impacted by mom's high school status. According to the plot below, For Mom who went to high school, the kid_score distribution on average shifts a bit higher and the density for those with higher test scores seems to be higher as well. For example, when test score is 100, the density for kid's whose mom went to high scholl has a higher test scores than those who didn't.

```{r}
kidiq$mom_hs <- as.character(kidiq$mom_hs)
ggplot(kidiq, aes(x=kid_score, color = mom_hs, fill=mom_hs)) + 
  geom_density(alpha=0.3) +
  labs(x = "Kid's Score", y="Density", title = "Distribution of kid's test score by if mom went to high school high school")
```

Then it would be interesting to explore the relationship between mother's IQ and kid's score. From the scatter plot below we see that there seems to have a positive relationship between the two. An increase in Mother's IQ will likely result in an increase in kid's test score. 

```{r}
ggplot(kidiq, aes(x=mom_iq, y=kid_score)) +
  geom_point() + 
  geom_smooth() +
  labs(x = "Mother IQ", y="Kid's Score", title = "The relationship betweem kid's test score and mother's IQ")
```

3 - The relationship between the kid's test score and the mother's age  

Next I explored whether there are some relationships between the kid's test score and the mother's age. This curve is rather flat, indicating that there is no clear relationship that these two variables are related in anyay. 

```{r}
ggplot(kidiq, aes(x=mom_age, y=kid_score)) +
  geom_point(position = "jitter", alpha=0.5, shape = 16, color="red") + 
  geom_smooth() +
  labs(x = "Mother's Age", y="Kid's Score", title = "The relationship betweem kid's score and mom's age")
```


# Estimating mean, no covariates

In class we were trying to estimate the mean and standard deviation of the kid's test scores. The `kids2.stan` file contains a Stan model to do this. If you look at it, you will notice the first `data` chunk lists some inputs that we have to define: the outcome variable `y`, number of observations `N`, and the mean and standard deviation of the prior on `mu`. Let's define all these values in a `data` list.

$$
y_i |\mu, \sigma \sim N(\mu, \sigma^2)
$$
priors:
$$
\sigma \sim N^+(0,10^2)
$$
$$
\mu \sim N(\mu_0, \sigma^2_0)
$$


```{r}
y <- kidiq$kid_score
mu0 <- 80
sigma0 <- 10

# named list to input for stan function
data <- list(y = y, 
             N = length(y), 
             mu0 = mu0,
             sigma0 = sigma0)
```



Now we can run the model:

```{r}
fit <- stan(file = here("kids2.stan"),
            data = data,
            # reducing the iterations a bit to speed things up
            chains = 3,
            iter = 500)
```

Look at the summary

```{r}
fit
```

Traceplot

```{r}
traceplot(fit)
```

All looks fine. 

```{r}
pairs(fit, pars = c("mu", "sigma"))
```

```{r}
stan_dens(fit, separate_chains = TRUE)
```


## Understanding output

What does the model actually give us? A number of samples from the posteriors. To see this, we can use `extract` to get the samples. 

```{r}
post_samples <- extract(fit)
names(post_samples)
head(post_samples[["mu"]])
```


This is a list, and in this case, each element of the list has 4000 samples. E.g. quickly plot a histogram of mu

```{r}
hist(post_samples[["mu"]])
median(post_samples[["mu"]])
# 95% bayesian credible interval
quantile(post_samples[["mu"]], 0.025)
quantile(post_samples[["mu"]], 0.975)
```

Tidybayes is also very useful:

```{r}
fit |> 
  gather_draws(mu, sigma) |> 
  median_qi(.width = 0.8)
```


## Plot estimates

There are a bunch of packages, built-in functions that let you plot the estimates from the model, and I encourage you to explore these options (particularly in `bayesplot`, which we will most likely be using later on). I like using the `tidybayes` package, which allows us to easily get the posterior samples in a tidy format (e.g. using gather draws to get in long format). Once we have that, it's easy to just pipe and do ggplots as usual. 


Get the posterior samples for mu and sigma in long format:

```{r}
dsamples <- fit  |> 
  gather_draws(mu, sigma) # gather = long format
dsamples

# wide format
fit  |>  spread_draws(mu, sigma)

# quickly calculate the quantiles using 

dsamples |> 
  median_qi(.width = 0.8)
```

Let's plot the density of the posterior samples for mu and add in the prior distribution

```{r}
dsamples |> 
  filter(.variable == "mu") |> 
  ggplot(aes(.value, color = "posterior")) + geom_density(size = 1) + 
  xlim(c(70, 100)) + 
  stat_function(fun = dnorm, 
        args = list(mean = mu0, 
                    sd = sigma0), 
        aes(colour = 'prior'), size = 1) +
  scale_color_manual(name = "", values = c("prior" = "red", "posterior" = "black")) + 
  ggtitle("Prior and posterior for mean test scores") + 
  xlab("score")
  
```

## Question 2

Change the prior to be much more informative (by changing the standard deviation to be 0.1). Rerun the model. Do the estimates change? Plot the prior and posterior densities. 


```{r}
y <- kidiq$kid_score
mu0 <- 80

# sd 0.1
sigma1 <- 0.1

data1 <- list(y = y, 
             N = length(y), 
             mu0 = mu0,
             sigma0 = sigma1)

fit1 <- stan(file = here("kids2.stan"),
            data = data1,
            chains = 3,
            iter = 2000)
```

```{r}
summary(fit1)$summary
```

We see that the estimation does not change much compared to the first one.

```{r}
y <- kidiq$kid_score
mu0 <- 80

sigma1 <- 0.1

dsamples1 <- fit1   %>% 
  gather_draws(mu, sigma) # gather = long format

dsamples1  %>% 
  filter(.variable == "mu")  %>% 
  ggplot(aes(.value, color = "posterior")) + geom_density(size = 1) + 
  xlim(c(75, 85)) + 
  stat_function(fun = dnorm, 
        args = list(mean = mu0, 
                    sd = sigma1), 
        aes(colour = 'prior'), size = 1) +
  scale_color_manual(name = "", values = c("prior" = "red", "posterior" = "black")) + 
  ggtitle("Prior and posterior for mu") + 
  xlab("Score")


dsamples1  %>% 
  filter(.variable == "sigma")  %>% 
  ggplot(aes(.value, color = "posterior")) + geom_density(size = 1) + 
  xlim(c(-30, 30)) + 
  stat_function(fun = dnorm, 
        args = list(mean = 0, 
                    sd = 10), 
        aes(colour = 'prior'), size = 1) +
  scale_color_manual(name = "", values = c("prior" = "blue", "posterior" = "black")) + 
  ggtitle("Prior and posterior for Sigma") + 
  xlab("Score")
  
```
Here we see the big difference! We see that with this more informative prior, it is evident that the posterior distribution for mu becomes more closer to the piror, indicating a better fit. The posterior shape for sigma is changed as well although the prior is on mu only. This tells us that getting informative priors is really important when fitting a bayesian model. 

# Adding covariates

Now let's see how kid's test scores are related to mother's education. We want to run the simple linear regression

$$
y_i|\mu_i, \sigma^2 \sim N(\mu_i, \sigma^2)
$$



$$
\mu_i = \alpha + \beta X_i
$$
Priors:
$$
\alpha \sim N(0, 100^2)
$$
$$
\beta\sim N(0, 10^2)
$$
$$
\sigma \sim N(0, 10^2)
$$


where $X = 1$ if the mother finished high school and zero otherwise. 

`kid3.stan` has the stan model to do this. Notice now we have some inputs related to the design matrix $X$ and the number of covariates (in this case, it's just 1).

Let's get the data we need and run the model. 



```{r}
kidiq <- read_rds("kidiq.RDS")
X <- as.matrix(kidiq$mom_hs, ncol = 1) # force this to be a matrix
K <- 1

data <- list(y = y, N = length(y), 
             X =X, K = K)
fit2 <- stan(file = here("kids3.stan"),
            data = data, 
            iter = 5000)

fit2
traceplot(fit2)
```

## Question 3

a) Confirm that the estimates of the intercept and slope are comparable to results from `lm()` 


```{r}
summary(fit)$summary

# lm
model2 <- lm(kidiq$kid_score ~ kidiq$mom_hs)
summary(model2)

summary(fit2)$summary[1:2,1]
summary(model2)$coefficients[,"Estimate"]
```

The two methods yields similar coeeficients estimates as shown above. The Bayesian posterior mean for mu is 86.68008 and the Bayesian posterior mean for sigma is 20.37315. The residual standard error (an estimate of the standard deviation of the error term, sigma in Bayesian terms) from lm is 19.85, similar to bayesian.

b) Do a `pairs` plot to investigate the joint sample distributions of the slope and intercept. Comment briefly on what you see. Is this potentially a problem?  

```{r}
pairs(fit2, pars = c("alpha", "beta"))
```
Well the above plot shows that there is a negative correlation relationship between the slope and the intercept. As the intercepts gets larger, the slop becomes smaller. The correlation between the two should be close to -1. This means that there would be a slight problem as a small change in slope can change the intercept and thus making it harder to sample. 

## Plotting results

It might be nice to plot the posterior samples of the estimates for the non-high-school and high-school mothered kids. Here's some code that does this: notice the `beta[condition]` syntax. Also notice I'm using `spread_draws`, because it's easier to calculate the estimated effects in wide format


## Question 4

Add in mother's IQ as a covariate and rerun the model. Please  mean center the covariate before putting it into the model. Interpret the coefficient on the (centered) mum's IQ. 

```{r}
kidiq$mom_iq_meanadj <- kidiq$mom_iq - mean(kidiq$mom_iq)

X <- cbind(kidiq$mom_hs, kidiq$mom_iq_meanadj)

y <- kidiq$kid_score  # Assuming kid_score is the response variable
data3 <- list(y = y, N = length(y), X = X, K = ncol(X))

```

```{r}
fit3 <- stan(file = here("kids3.stan"),
            data = data3, 
            iter = 5000)
```

Here is a summary of the model:  

```{r}
summary(fit3)$summary
```

This model implicate the following observations:
1. We see that the coefficient of the mean_centered IQ is approximately 0.565, which means that given all other variables unchanged, the kid's test score will likely increase by 0.565 points if mom's IQ rise by 1 unit. 
2. The intercept shows that the base test score that the kid will have when no high school mom and a mean IQ. 


## Question 5 

Confirm the results from Stan agree with `lm()`

```{r}
summary(fit3)$summary

model3 <- lm(kidiq$kid_score ~ kidiq$mom_hs + kidiq$mom_iq_meanadj)
summary(model3)

summary(fit3)$summary[1:3,1]
summary(model3)$coefficients[,"Estimate"]
```

Well we see that the coefficient from the Bayesian model and the MLE estimator are pretty similar, so is the case with the standardized error. The stan model result is confirmed with the lm() approach.

## Question 6

Plot the posterior estimates of scores by education of mother for mothers who have an IQ of 110. 

```{r}
110-mean(kidiq$mom_iq)
```

```{r}
fit3 %>%
  spread_draws(alpha, beta[k], sigma) %>% 
  pivot_wider(names_from = k, names_prefix = "beta", values_from = beta) %>%  # Transforms beta coefficients
  mutate(
    nhs = alpha + beta2 * 10,  # Calculates the estimated score for non-high school graduates with the other covariate at +10, since average mom IQ is deviate from 110 by 10
    hs = alpha + beta1 + beta2 * 10  # Calculates the estimated score for high school graduates under the same conditions.
  ) %>% 
  select(nhs, hs) %>%  
  pivot_longer(nhs:hs, names_to = "education", values_to = "estimated_score") %>%  
  ggplot(aes(y = education, x = estimated_score)) + 
  stat_halfeye() + 
  theme_bw() +
  ggtitle("Posterior estimates of scores by education level of mother with IQ 110") 
```

## Question 7

Generate and plot (as a histogram) samples from the posterior predictive distribution for a new kid with a mother who graduated high school and has an IQ of 95. 

```{r}
x_diff <- 95- mean(kidiq$mom_iq) 

post_samples3 <- extract(fit3)
sigma <- post_samples3$sigma

pred <- post_samples3$alpha + post_samples3$beta[,1] + post_samples3$beta[,2] *x_diff

new_sample <- rnorm(length(sigma), mean = pred, sd = sigma)

hist(new_sample, main="Histogram of kid's test score")
```

